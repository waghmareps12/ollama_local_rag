{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# subprocess.run(\"ollama serve\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "# #Then everytime you want to chat\n",
    "# response = ollama.chat(model='llama3.2', messages=[\n",
    "#   {\n",
    "#     'role': 'user',\n",
    "#     'content': 'Why is the sky blue?',\n",
    "#   },\n",
    "# ])\n",
    "# print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import AzureOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() # read local .env file\n",
    "OPENAI_API_BASE = os.getenv('OPENAI_API_BASE')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "OPENAI_API_BASE = os.getenv('OPENAI_API_BASE')\n",
    "OPENAI_API_VERSION = os.getenv('OPENAI_API_VERSION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "# from streamlit_extras.add_vertical_space import add_vertical_space\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "# from langchain_openai import AzureOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = AzureOpenAIEmbeddings(\n",
    "#     model=\"text-embedding-3-large\",\n",
    "#     deployment= 'genai-gpt-35-turbo',\n",
    "#     # dimensions: Optional[int] = None, # Can specify dimensions with new text-embedding-3 models\n",
    "#     azure_endpoint=OPENAI_API_BASE, #\"https://<your-endpoint>.openai.azure.com/\", If not provided, will read env variable AZURE_OPENAI_ENDPOINT\n",
    "#     api_key=OPENAI_API_KEY, # Can provide an API key directly. If missing read env variable AZURE_OPENAI_API_KEY\n",
    "#     openai_api_version=OPENAI_API_VERSION, # If not provided, will read env variable AZURE_OPENAI_API_VERSION\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to process text data in chunks and add to the FAISS index\n",
    "# def process_in_chunks(text_data, chunk_size, embeddings, index_name):\n",
    "#     # Check if the index already exists\n",
    "#     if os.path.exists(f\"{index_name}.faiss\") and os.path.exists(f\"{index_name}.pkl\"):\n",
    "#         vectorstore = FAISS.load_local(index_name, embeddings)\n",
    "#         print(\"Loaded existing FAISS index.\")\n",
    "#     else:\n",
    "#         vectorstore = None\n",
    "    \n",
    "#     for i in range(0, len(text_data), chunk_size):\n",
    "#         chunk = text_data[i:i + chunk_size]\n",
    "#         try:\n",
    "#             print(f\"Processing chunk {i // chunk_size + 1}\")\n",
    "#             temp_vectorstore = FAISS.from_texts(chunk, embeddings)\n",
    "            \n",
    "#             if vectorstore:\n",
    "#                 vectorstore.merge_from(temp_vectorstore)\n",
    "#             else:\n",
    "#                 vectorstore = temp_vectorstore\n",
    "            \n",
    "#             # Save the updated FAISS index after processing each chunk\n",
    "#             vectorstore.save_local(index_name)\n",
    "#             print(f\"Chunk {i // chunk_size + 1} processed and index saved.\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing chunk {i}: {e}\")\n",
    "        \n",
    "#         time.sleep(1)  # Sleep to respect the rate limit\n",
    "    \n",
    "#     return vectorstore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory loaded\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=False)\n",
    "\n",
    "# Load documents\n",
    "loader = DirectoryLoader(\"./input-data\", glob=\"**/*.txt\")\n",
    "print(\"Directory loaded\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=300,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created successfully\n",
      "FAISS index saved locally\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS index\n",
    "vectorstore = FAISS.from_documents(texts, embeddings,)\n",
    "\n",
    "print(\"FAISS index created successfully\")\n",
    "\n",
    "# Save the index locally\n",
    "vectorstore.save_local(\"my_faiss_index\")\n",
    "\n",
    "print(\"FAISS index saved locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved index\n",
    "loaded_vectorstore = FAISS.load_local(\"my_faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Create retriever\n",
    "retriever = loaded_vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # read local .env file\n",
    "OPENAI_API_TYPE = os.getenv('OPENAI_API_TYPE')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "OPENAI_API_BASE = os.getenv('OPENAI_API_BASE')\n",
    "OPENAI_API_VERSION = os.getenv('OPENAI_API_VERSION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\750040697\\AppData\\Local\\Temp\\ipykernel_18016\\3925094818.py:1: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import AzureChatOpenAI``.\n",
      "  llm = AzureChatOpenAI(openai_api_base=OPENAI_API_BASE,\n"
     ]
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(openai_api_base=OPENAI_API_BASE,\n",
    "                        openai_api_version=OPENAI_API_VERSION,\n",
    "                        openai_api_key=OPENAI_API_KEY,\n",
    "                        openai_api_type=OPENAI_API_TYPE,\n",
    "                        #deployment_name = 'genai-gpt-4-32k',\n",
    "                        #model_name = 'gpt-4-32k')\n",
    "                        deployment_name = 'genai-gpt-35-turbo',\n",
    "                        model_name = 'gpt-35-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create prompt template\n",
    "template = \"\"\"<bos><start_of_turn>user\n",
    "You are a sales inquiry assistant specializing in rebates, market share, promotions, and payouts. Answer the question based only on the following context and provide a meaningful response. \n",
    "Write in full sentences with correct spelling and punctuation. Use bullet points or numbered lists when appropriate.\n",
    "If the context doesn't contain the answer, politely state that you don't have the specific information and suggest contacting the sales department for more details.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "ANSWER:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template )\n",
    "\n",
    "# # Modify the rag_chain to return the result instead of streaming\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All affiliation change requests must be accompanied by documentation (such as an email) from the account requesting such. Please include account number, group to be affiliated with, and quarter to begin new affiliation. Alternatively, the account can send the request to RA VISUS SalesInquir@its.jnj.com directly. Ensure that they include SAP, date of affiliation change, and group to be affiliated with. An account must join, or have joined, the requested RfP group on or before the last day of the first month of the quarter to qualify for a rebate.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"RfP affiliation change request\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'./car_dummy.csv')\n",
    "df_columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agent = create_pandas_dataframe_agent(llm, df, verbose=True,allow_dangerous_code=True,max_iterations = 3)\n",
    "\n",
    "# Create embeddings for the vector store\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=False)\n",
    "\n",
    "# Load the saved index\n",
    "loaded_vectorstore = FAISS.load_local(\"my_faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Create retriever\n",
    "retriever = loaded_vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"<bos><start_of_turn>user\n",
    "You are a sales inquiry assistant specializing in rebates, market share, promotions, and payouts. Answer the question based on the following context, including retrieved documents and relevant employee data when applicable. \n",
    "Write in full sentences with correct spelling and punctuation. Use bullet points or numbered lists when appropriate.\n",
    "If the context doesn't contain the specific information, politely state that you don't have the answer and suggest contacting the sales department for further details.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "DATAFRAME INFO: {dataframe_info} dataframe columns = {df_columns}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "ANSWER:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to check if a question is related to structured data in the DataFrame\n",
    "# def is_dataframe_question(query):\n",
    "#     related_keywords = ['rebate', 'market share', 'promotion', 'payout']\n",
    "#     return any(keyword in query.lower() for keyword in related_keywords)\n",
    "\n",
    "# # Main chatbot logic\n",
    "# def chatbot(query):\n",
    "#     # Retrieve relevant documents from the vector store (always needed)\n",
    "#     context = retriever.get_relevant_documents(query)\n",
    "#     context_str = \"\\n\".join([doc.page_content for doc in context])\n",
    "\n",
    "#     # Check if the query is related to the pandas DataFrame\n",
    "#     if is_dataframe_question(query):\n",
    "#         # Query the pandas DataFrame agent\n",
    "#         dataframe_info = df_agent.run(query)\n",
    "#     else:\n",
    "#         # If it's not related to the DataFrame, leave dataframe_info empty\n",
    "#         dataframe_info = \"No relevant structured data found.\"\n",
    "\n",
    "#     # Create input for the LLM with both context and DataFrame info\n",
    "#     final_input = template.format(context=context_str, dataframe_info=dataframe_info, question=query)\n",
    "\n",
    "#     # Get response from the LLM\n",
    "#     response = llm_for_pandas(final_input)\n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a question is related to structured data in the DataFrame\n",
    "# def is_dataframe_question(query):\n",
    "#     related_keywords = ['rebate', 'market share', 'promotion', 'payout']\n",
    "#     return any(keyword in query.lower() for keyword in related_keywords)\n",
    "\n",
    "# Main chatbot logic\n",
    "def chatbot(query):\n",
    "    # Retrieve relevant documents from the vector store (always needed)\n",
    "    context = retriever.get_relevant_documents(query)\n",
    "    context_str = \"\\n\".join([doc.page_content for doc in context])\n",
    "\n",
    "    # # Check if the query is related to the pandas DataFrame\n",
    "    # if is_dataframe_question(query):\n",
    "        # Query the pandas DataFrame agent\n",
    "    \n",
    "    # Always run the pandas DataFrame agent query, with error handling\n",
    "    try:\n",
    "        dataframe_info = df_agent.run(query)\n",
    "    except Exception as e:\n",
    "        # Handle the error and provide a default response\n",
    "        dataframe_info = f\"No relevant structured data found in DataFrame\"\n",
    "\n",
    "    # Create input for the LLM with both context and DataFrame info\n",
    "    final_input = template.format(context=context_str, dataframe_info=dataframe_info, df_columns= df_columns, question=query)\n",
    "\n",
    "    # Get response from the LLM\n",
    "    response = llm.invoke(final_input)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: We need more information to answer this question. What account are we looking for? Is it in a specific column of the dataframe?\n",
      "Action: Check if a specific account is in the `RFP_Key` column of the dataframe\n",
      "Action Input: `df[df['RFP_Key'] == 'account number']`\u001b[0mCheck if a specific account is in the `RFP_Key` column of the dataframe is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3mWe need to find all unique values in the `RFP_Key` column to see if the account is present.\n",
      "Action: Use the `unique()` method to get all unique values in the `RFP_Key` column\n",
      "Action Input: `df['RFP_Key'].unique()`\u001b[0mUse the `unique()` method to get all unique values in the `RFP_Key` column is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3mWe need to search for the account in the `RFP_Key` column using a boolean expression.\n",
      "Action: Use boolean indexing to check if the account is present in the `RFP_Key` column\n",
      "Action Input: `df[df['RFP_Key'] == 'account number']`\u001b[0mUse boolean indexing to check if the account is present in the `RFP_Key` column is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='An account will not appear on the RfP Tracker if it has $0 in sales for the current quarter or if it is not an active member of any RfP group. There are known issues when searching by account name in the RfP tracker. Please try searching by account number. If the issue persists, please contact the sales department for further assistance.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 1763, 'total_tokens': 1837, 'completion_tokens_details': None}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a65e5301-9e2d-4a94-a1a1-613a42a8989c-0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('Account is not found on the RfP Tracker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
