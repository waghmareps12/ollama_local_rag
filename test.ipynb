{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# subprocess.run(\"ollama serve\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "# #Then everytime you want to chat\n",
    "# response = ollama.chat(model='llama3.2', messages=[\n",
    "#   {\n",
    "#     'role': 'user',\n",
    "#     'content': 'Why is the sky blue?',\n",
    "#   },\n",
    "# ])\n",
    "# print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import AzureOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() # read local .env file\n",
    "OPENAI_API_BASE = os.getenv('OPENAI_API_BASE')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "OPENAI_API_BASE = os.getenv('OPENAI_API_BASE')\n",
    "OPENAI_API_VERSION = os.getenv('OPENAI_API_VERSION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "# from streamlit_extras.add_vertical_space import add_vertical_space\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "# from langchain_openai import AzureOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    deployment= 'genai-gpt-35-turbo',\n",
    "    # dimensions: Optional[int] = None, # Can specify dimensions with new text-embedding-3 models\n",
    "    azure_endpoint=OPENAI_API_BASE, #\"https://<your-endpoint>.openai.azure.com/\", If not provided, will read env variable AZURE_OPENAI_ENDPOINT\n",
    "    api_key=OPENAI_API_KEY, # Can provide an API key directly. If missing read env variable AZURE_OPENAI_API_KEY\n",
    "    openai_api_version=OPENAI_API_VERSION, # If not provided, will read env variable AZURE_OPENAI_API_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to process text data in chunks and add to the FAISS index\n",
    "# def process_in_chunks(text_data, chunk_size, embeddings, index_name):\n",
    "#     # Check if the index already exists\n",
    "#     if os.path.exists(f\"{index_name}.faiss\") and os.path.exists(f\"{index_name}.pkl\"):\n",
    "#         vectorstore = FAISS.load_local(index_name, embeddings)\n",
    "#         print(\"Loaded existing FAISS index.\")\n",
    "#     else:\n",
    "#         vectorstore = None\n",
    "    \n",
    "#     for i in range(0, len(text_data), chunk_size):\n",
    "#         chunk = text_data[i:i + chunk_size]\n",
    "#         try:\n",
    "#             print(f\"Processing chunk {i // chunk_size + 1}\")\n",
    "#             temp_vectorstore = FAISS.from_texts(chunk, embeddings)\n",
    "            \n",
    "#             if vectorstore:\n",
    "#                 vectorstore.merge_from(temp_vectorstore)\n",
    "#             else:\n",
    "#                 vectorstore = temp_vectorstore\n",
    "            \n",
    "#             # Save the updated FAISS index after processing each chunk\n",
    "#             vectorstore.save_local(index_name)\n",
    "#             print(f\"Chunk {i // chunk_size + 1} processed and index saved.\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing chunk {i}: {e}\")\n",
    "        \n",
    "#         time.sleep(1)  # Sleep to respect the rate limit\n",
    "    \n",
    "#     return vectorstore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory loaded\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=False)\n",
    "\n",
    "# Load documents\n",
    "loader = DirectoryLoader(\"./input-data\", glob=\"**/*.txt\")\n",
    "print(\"Directory loaded\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=300,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created successfully\n",
      "FAISS index saved locally\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS index\n",
    "vectorstore = FAISS.from_documents(texts, embeddings,)\n",
    "\n",
    "print(\"FAISS index created successfully\")\n",
    "\n",
    "# Save the index locally\n",
    "vectorstore.save_local(\"my_faiss_index\")\n",
    "\n",
    "print(\"FAISS index saved locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved index\n",
    "loaded_vectorstore = FAISS.load_local(\"my_faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Create retriever\n",
    "retriever = loaded_vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # read local .env file\n",
    "OPENAI_API_TYPE = os.getenv('OPENAI_API_TYPE')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "OPENAI_API_BASE = os.getenv('OPENAI_API_BASE')\n",
    "OPENAI_API_VERSION = os.getenv('OPENAI_API_VERSION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\750040697\\AppData\\Local\\Temp\\ipykernel_1944\\3925094818.py:1: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import AzureChatOpenAI``.\n",
      "  llm = AzureChatOpenAI(openai_api_base=OPENAI_API_BASE,\n"
     ]
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(openai_api_base=OPENAI_API_BASE,\n",
    "                        openai_api_version=OPENAI_API_VERSION,\n",
    "                        openai_api_key=OPENAI_API_KEY,\n",
    "                        openai_api_type=OPENAI_API_TYPE,\n",
    "                        #deployment_name = 'genai-gpt-4-32k',\n",
    "                        #model_name = 'gpt-4-32k')\n",
    "                        deployment_name = 'genai-gpt-35-turbo',\n",
    "                        model_name = 'gpt-35-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create prompt template\n",
    "template = \"\"\"<bos><start_of_turn>user\n",
    "You are a sales inquiry assistant specializing in rebates, market share, promotions, and payouts. Answer the question based only on the following context and provide a meaningful response. \n",
    "Write in full sentences with correct spelling and punctuation. Use bullet points or numbered lists when appropriate.\n",
    "If the context doesn't contain the answer, politely state that you don't have the specific information and suggest contacting the sales department for more details.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "ANSWER:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template )\n",
    "\n",
    "# # Modify the rag_chain to return the result instead of streaming\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All affiliation change requests must be accompanied by documentation (such as an email) from the account requesting such. Please include account number, group to be affiliated with, and quarter to begin new affiliation. Alternatively, the account can send the request to RA VISUS SalesInquir@its.jnj.com directly. Ensure that they include SAP, date of affiliation change, and group to be affiliated with. An account must join, or have joined, the requested RfP group on or before the last day of the first month of the quarter to qualify for a rebate.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"RfP affiliation change request\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\750040697\\AppData\\Local\\Temp\\ipykernel_1944\\4169773792.py:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  relevant_documents = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 relevant documents\n",
      "RfP Alignments and Affiliations\n",
      "\n",
      "Account is not found on the RfP Tracker\n",
      "\n",
      "An account will not appear...\n",
      "VISUS\n",
      "\n",
      "SalesInquir@its.jnj.com directly. Ensure that they include SAP, date of affiliation change, a...\n",
      "You may view quarterly RFP history in the Preferred RFP Tracker in the ‘Rebate History’ section. ACU...\n",
      "If it is myAPP, Alliance or LF ; Refer to the CX team\n",
      "\n",
      "Option to Send Inquiry\n",
      "\n",
      "Name or address on pa...\n",
      "Rebate Percentage Errors in the Rfp Tracker This is only a reporting issue in the Rfp tracker and di...\n"
     ]
    }
   ],
   "source": [
    "# Now you can use the retriever in your RAG pipeline\n",
    "# # For example:\n",
    "# query = \"RfP affiliation change request\"\n",
    "# relevant_documents = retriever.get_relevant_documents(query)\n",
    "\n",
    "# print(f\"Retrieved {len(relevant_documents)} relevant documents\")\n",
    "# for doc in relevant_documents:\n",
    "#     print(doc.page_content[:100] + \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
